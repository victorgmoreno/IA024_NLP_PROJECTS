{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A4HuVIYGgXg"
      },
      "source": [
        "# Alunos Regulares IA-024-2024S1 FEEC-UNICAMP\n",
        "versão 26 de fevereiro de 2024, 19h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRnOFwqk23W4"
      },
      "source": [
        "## Instalação e importação de pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA5BWLDCKmw3",
        "outputId": "ee6d5cf3-9e70-49c4-a3ac-00b31b668a3e"
      },
      "outputs": [],
      "source": [
        "#!pip install torchtext\n",
        "#!pip install 'portalocker>=2.0.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VorDvF62iyXF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.datasets import IMDB\n",
        "from collections import Counter\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5ovJE02CwKT"
      },
      "source": [
        "## I - Vocabulário e Tokenização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mqzUqy3diz0X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12500 são positivas, 12500 são negativas e existe um total de 25000 amostras.\n",
            "O comprimento é médio de palavras por linha é de 233.7872 palavras.\n",
            "As cinco palavras mais frequentes são ['the', 'and', 'a', 'of', 'to']\n",
            "As cinco palavras menos frequentes são ['goring', 'cacoyannis', 'showings', 'dolores', 'ponderosa']\n"
          ]
        }
      ],
      "source": [
        "# limit the vocabulary size to 20000 most frequent tokens\n",
        "vocab_size = 20000\n",
        "\n",
        "negative = 0\n",
        "positive = 0\n",
        "average_text_length = 0\n",
        "train_set = []\n",
        "\n",
        "counter = Counter()\n",
        "for (target, line) in list(IMDB(split='train')):\n",
        "    \n",
        "    # Counting how many positive or negative analysis sample and ]\n",
        "    average_text_length = len(line.split()) + average_text_length\n",
        "    if target == 1:\n",
        "        negative += 1\n",
        "    elif target == 2:\n",
        "        positive += 1\n",
        "    line = line.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "    counter.update(line.split())\n",
        "    train_set.append(line)\n",
        "\n",
        "# create a vocabulary of the 20000 most frequent tokens\n",
        "most_frequent_words = sorted(counter, key=counter.get, reverse=True)[:vocab_size]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f'{positive} são positivas, {negative} são negativas e existe um total de {positive + negative} amostras.')\n",
        "print(f'O comprimento é médio de palavras por linha é de {average_text_length/len(list(IMDB(split='train')))} palavras.')\n",
        "print(f'As cinco palavras mais frequentes são {most_frequent_words[:5]}')\n",
        "print(f'As cinco palavras menos frequentes são {most_frequent_words[-1:-6: -1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rZn-m1Mi110",
        "outputId": "7a859e5f-b468-42f0-b7f7-b3c11455bea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 38, 0]\n",
            "0 é a codificação que atribui o código de \"unknown token\" para palavras desconhecidas. Caso da palavra \"Pizza\", que não está no vocab.\n",
            "No dataset de treino, existem 214473 unknown tokens.\n"
          ]
        }
      ],
      "source": [
        "unk_words_length = 0\n",
        "\n",
        "def encode_sentence(sentence, vocab):\n",
        "    return [vocab.get(word, 0) for word in sentence.split()] # 0 for OOV\n",
        "\n",
        "print(encode_sentence(\"I like Pizza.\", vocab))\n",
        "print('0 é a codificação que atribui o código de \"unknown token\" para palavras desconhecidas. Caso da palavra \"Pizza\", que não está no vocab.')\n",
        "\n",
        "for sentence in train_set:\n",
        "    unk_words_length += encode_sentence(sentence, vocab).count(0)\n",
        "\n",
        "print(f'No dataset de treino, existem {unk_words_length} unknown tokens.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A quantidade de classes 1 no dataset de 200 elementos é 200\n",
            "A razão pelo qual a precisão se aproxima de 100% é porque o conjunto de treinamento está composto apenas por amostras de uma única classe. Dessa forma, ele decorou como classificar um unico tipo de texto.\n",
            "Após o balanceamento, a quantidade de classes 1 no dataset de 200 elementos é 100\n"
          ]
        }
      ],
      "source": [
        "quantidade_1 = 0\n",
        "for i,_ in list(IMDB(split='train'))[:200]:\n",
        "    if i == 1:\n",
        "        quantidade_1 += 1\n",
        "    \n",
        "print(f'A quantidade de classes 1 no dataset de 200 elementos é {quantidade_1}')\n",
        "print('A razão pelo qual a precisão se aproxima de 100% é porque o conjunto de treinamento está composto apenas por amostras de uma única classe. Dessa forma, ele decorou como classificar um unico tipo de texto.')\n",
        "\n",
        "# Deixando o dataset balanceado.\n",
        "negative_train_data = [i for i in list(IMDB(split='train')) if i[0] == 1][:100]\n",
        "positive_train_data = [i for i in list(IMDB(split='train')) if i[0] == 2][:100]\n",
        "balanced_train_data = negative_train_data + positive_train_data\n",
        "\n",
        "quantidade_1 = 0\n",
        "for i,_ in balanced_train_data:\n",
        "    if i == 1:\n",
        "        quantidade_1 += 1\n",
        "print(f'Após o balanceamento, a quantidade de classes 1 no dataset de 200 elementos é {quantidade_1}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iV4bF8cDAj1"
      },
      "source": [
        "## II - Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VDUyZoTPi262"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"from torch.nn.functional import one_hot\\n# Dataset Class with One-hot Encoding\\nclass IMDBDataset(Dataset):\\n    def __init__(self, split, vocab):\\n        self.data = list(IMDB(split=split))\\n        self.vocab = vocab\\n\\n    def __len__(self):\\n        return len(self.data)\\n\\n    def __getitem__(self, idx):\\n        target, line = self.data[idx]\\n        target = 1 if target == 1 else 0\\n\\n        # one-hot encoding\\n        X = torch.zeros(len(self.vocab) + 1)\\n        for word in encode_sentence(line, self.vocab):\\n            X[word] = 1\\n\\n        return X, torch.tensor(target)\\n\\n# Load Data with One-hot Encoding\\ntrain_data = IMDBDataset('train', vocab)\\ntest_data = IMDBDataset('test', vocab)\\n\""
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Classe não otimizada\n",
        "\n",
        "\"\"\"from torch.nn.functional import one_hot\n",
        "# Dataset Class with One-hot Encoding\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, split, vocab):\n",
        "        self.data = list(IMDB(split=split))\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        target, line = self.data[idx]\n",
        "        target = 1 if target == 1 else 0\n",
        "\n",
        "        # one-hot encoding\n",
        "        X = torch.zeros(len(self.vocab) + 1)\n",
        "        for word in encode_sentence(line, self.vocab):\n",
        "            X[word] = 1\n",
        "\n",
        "        return X, torch.tensor(target)\n",
        "\n",
        "# Load Data with One-hot Encoding\n",
        "train_data = IMDBDataset('train', vocab)\n",
        "test_data = IMDBDataset('test', vocab)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, split, vocab):\n",
        "        self.data = list(IMDB(split=split))\n",
        "        self.vocab = vocab\n",
        "        self.one_hot_encoded_data = self.preprocess_data()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.one_hot_encoded_data[idx]\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        one_hot_encoded_data = []\n",
        "        for target, line in self.data:\n",
        "            target = 1 if target == 1 else 0\n",
        "            line = line.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "            # One-hot encoding\n",
        "            X = torch.zeros(len(self.vocab) + 1)\n",
        "            for word in encode_sentence(line, self.vocab):\n",
        "                X[word] = 1\n",
        "            one_hot_encoded_data.append((X, torch.tensor(target)))\n",
        "        return one_hot_encoded_data\n",
        "\n",
        "# Load Data with One-hot Encoding\n",
        "train_data = IMDBDataset('train', vocab)\n",
        "test_data = IMDBDataset('test', vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de amostras positivas, ao chamar o atributo do objeto (train_data.data): 12500\n",
            "Número de amostras negativas, ao chamar o atributo do objeto (train_data.data): 12500\n",
            "Número de amostras positivas, ao chamar apenas o objeto (train_data): 12500\n",
            "Número de amostras negativas, ao chamar apenas o objeto (train_data): 12500\n",
            "O número médio de palavras codificadas em cada vetor one-hot é de: 133.73216247558594\n",
            "Comprimento médio de cada texto (em palavras): 233.7872\n",
            "Existe a diferença da dimensão do comprimento médio entre o texto pois o onehot considera apenas as palavras que estão no vocabulário.\n"
          ]
        }
      ],
      "source": [
        "# Contadores para amostras positivas e negativas\n",
        "positive_samples = 0\n",
        "negative_samples = 0\n",
        "\n",
        "# Iterar sobre o conjunto de dados de treinamento\n",
        "for target, line in train_data.data:\n",
        "    # Incrementar o contador correspondente à classe da amostra\n",
        "    if target == 1:\n",
        "        positive_samples += 1\n",
        "    elif target == 2:\n",
        "        negative_samples += 1\n",
        "print(\"Número de amostras positivas, ao chamar o atributo do objeto (train_data.data):\", positive_samples)\n",
        "print(\"Número de amostras negativas, ao chamar o atributo do objeto (train_data.data):\", negative_samples)\n",
        "\n",
        "negative_samples = 0\n",
        "positive_samples = 0\n",
        "\n",
        "for i in train_data:\n",
        "    if i[1] == 1:\n",
        "        negative_samples += 1\n",
        "    elif i[1] == 0:\n",
        "        positive_samples += 1\n",
        "    \n",
        "print(\"Número de amostras positivas, ao chamar apenas o objeto (train_data):\", positive_samples)\n",
        "print(\"Número de amostras negativas, ao chamar apenas o objeto (train_data):\", negative_samples)\n",
        "\n",
        "\"\"\"Calculando o comprimento médio das palavras codificadas\"\"\"\n",
        "\n",
        "# Calculate the number of non-zero elements in each one-hot encoded vector\n",
        "\n",
        "\n",
        "counts = torch.tensor([torch.sum(X != 0).item() for X, _ in train_data])\n",
        "\n",
        "# Calculate the average number of words encoded in each one-hot vector\n",
        "average_non_zero_elements = torch.mean(counts.float()).item()\n",
        "\n",
        "print(f'O número médio de palavras codificadas em cada vetor one-hot é de: {average_non_zero_elements}')\n",
        "print(\"Comprimento médio de cada texto (em palavras):\", average_text_length / len(train_data))\n",
        "print('Existe a diferença da dimensão do comprimento médio entre o texto pois o onehot considera apenas as palavras que estão no vocabulário.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7RMPSvMDL5U"
      },
      "source": [
        "## III - Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Y7tcZv2YDIog"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "# define dataloaders\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_data,  batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número total de iterações no train_loader: 196\n",
            "Número de amostras do último batch: 40\n",
            "Média dos valores de R: 0.5001753826530612\n",
            "Valor médio de R: 0.500175416469574\n",
            "Valores de R para cada batch: [0.4609375, 0.5625, 0.484375, 0.53125, 0.515625, 0.46875, 0.4375, 0.5703125, 0.5078125, 0.53125, 0.5, 0.515625, 0.5546875, 0.421875, 0.5078125, 0.5, 0.5234375, 0.546875, 0.5, 0.5234375, 0.4765625, 0.484375, 0.6015625, 0.4921875, 0.4765625, 0.40625, 0.484375, 0.5234375, 0.5, 0.5546875, 0.546875, 0.453125, 0.53125, 0.5, 0.4375, 0.59375, 0.5078125, 0.578125, 0.5, 0.53125, 0.5703125, 0.5546875, 0.546875, 0.4375, 0.515625, 0.5625, 0.5, 0.5390625, 0.453125, 0.515625, 0.4765625, 0.4921875, 0.53125, 0.5703125, 0.484375, 0.515625, 0.4921875, 0.5078125, 0.546875, 0.484375, 0.515625, 0.4921875, 0.4921875, 0.5, 0.53125, 0.4453125, 0.5078125, 0.53125, 0.4375, 0.5234375, 0.5, 0.515625, 0.5234375, 0.4921875, 0.5, 0.5, 0.5234375, 0.4375, 0.5078125, 0.46875, 0.4375, 0.3828125, 0.5390625, 0.4765625, 0.5, 0.484375, 0.5390625, 0.4921875, 0.4765625, 0.4765625, 0.53125, 0.4765625, 0.46875, 0.46875, 0.4375, 0.4609375, 0.4296875, 0.515625, 0.4765625, 0.5078125, 0.5078125, 0.578125, 0.5234375, 0.546875, 0.4140625, 0.4921875, 0.5078125, 0.5, 0.6015625, 0.546875, 0.4609375, 0.5078125, 0.3984375, 0.4453125, 0.4921875, 0.4375, 0.515625, 0.5078125, 0.421875, 0.5234375, 0.46875, 0.5078125, 0.484375, 0.4921875, 0.546875, 0.484375, 0.609375, 0.4921875, 0.578125, 0.578125, 0.4765625, 0.4609375, 0.515625, 0.484375, 0.484375, 0.5703125, 0.4921875, 0.5390625, 0.4609375, 0.5078125, 0.484375, 0.453125, 0.4609375, 0.484375, 0.515625, 0.5078125, 0.421875, 0.421875, 0.4921875, 0.515625, 0.5, 0.453125, 0.484375, 0.4609375, 0.4921875, 0.5390625, 0.46875, 0.515625, 0.5, 0.484375, 0.4921875, 0.453125, 0.4609375, 0.546875, 0.46875, 0.546875, 0.515625, 0.5703125, 0.4921875, 0.5, 0.4375, 0.5078125, 0.546875, 0.453125, 0.515625, 0.578125, 0.5234375, 0.5234375, 0.5703125, 0.4765625, 0.5078125, 0.515625, 0.4765625, 0.4375, 0.5, 0.5078125, 0.5234375, 0.5546875, 0.4921875, 0.3984375, 0.4609375, 0.46875, 0.4609375, 0.4921875, 0.5234375, 0.55]\n",
            "Shape dos inputs: torch.Size([128, 20001])\n",
            "Tipo de dado dos inputs: torch.float32\n",
            "Shape dos targets: torch.Size([128])\n",
            "Tipo de dado dos targets: torch.int64\n"
          ]
        }
      ],
      "source": [
        "# Número total de iterações no train_loader\n",
        "print(\"Número total de iterações no train_loader:\", len(train_loader))\n",
        "\n",
        "# Imprimindo o número de amostras do último batch\n",
        "print(f\"Número de amostras do último batch: {len(train_loader.dataset) % batch_size}\")\n",
        "\n",
        "# Calculando a relação R para cada batch usando list comprehension\n",
        "R_values = [torch.sum(targets).item() / len(targets) for inputs, targets in train_loader]\n",
        "\n",
        "# Calculando a média dos valores de R\n",
        "average_R = sum(R_values) / len(R_values)\n",
        "print(\"Média dos valores de R:\", average_R)\n",
        "\n",
        "# Calculando o valor médio de R usando tensores do PyTorch\n",
        "mean_R = torch.mean(torch.tensor(R_values))\n",
        "print(\"Valor médio de R:\", mean_R.item())\n",
        "\n",
        "# Lista de valores de R para cada batch\n",
        "print(\"Valores de R para cada batch:\", R_values)\n",
        "\n",
        "# Obtendo um batch do train_loader\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "# Extraindo os elementos do batch\n",
        "inputs, targets = batch\n",
        "\n",
        "# Mostrando a estrutura do batch\n",
        "print(\"Shape dos inputs:\", inputs.shape)\n",
        "print(\"Tipo de dado dos inputs:\", inputs.dtype)\n",
        "print(\"Shape dos targets:\", targets.shape)\n",
        "print(\"Tipo de dado dos targets:\", targets.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwPeJ7h8DahT"
      },
      "source": [
        "## IV - Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6QuDhWvji7lt"
      },
      "outputs": [],
      "source": [
        "class OneHotMLP(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(OneHotMLP, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(vocab_size+1, 200)\n",
        "        self.fc2 = nn.Linear(200, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        o = self.fc1(x.float())\n",
        "        o = self.relu(o)\n",
        "        return self.fc2(o)\n",
        "\n",
        "# Model instantiation\n",
        "model = OneHotMLP(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilidade: aleatoria 0.5125\n",
            "Valor da Loss: 0.6891483664512634\n",
            "Valor da Loss utilizando BCELoss: 0.6896375417709351\n",
            "Valor da Loss utilizando BCEWithLogitsLoss: 0.6896375417709351\n"
          ]
        }
      ],
      "source": [
        "inputs,targets = next(iter(train_loader))\n",
        "logits = model(inputs)\n",
        "probabilities = torch.sigmoid(logits)\n",
        "\n",
        "# Imprimir as probabilidades\n",
        "for probability in probabilities:\n",
        "    print(f'Probabilidade: aleatoria {probability.item():.4f}')\n",
        "    break\n",
        "    \n",
        "# Calcular a Loss utilizando a função de entropia cruzada\n",
        "loss = - (targets * torch.log(probabilities) + (1 - targets) * torch.log(1 - probabilities))\n",
        "total_loss = torch.mean(loss)\n",
        "print(\"Valor da Loss:\", total_loss.item())\n",
        "\n",
        "# Instanciar a função de Loss BCELoss e BCEWithLogitsLoss\n",
        "bce_loss_function = nn.BCELoss()\n",
        "bce_with_logits_loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "# Expandir as dimensões do alvo para corresponder à dimensão da entrada\n",
        "targets_expanded = targets.unsqueeze(1).float()\n",
        "\n",
        "# Calcular a Loss utilizando a função de BCELoss\n",
        "loss_bce = bce_loss_function(probabilities, targets_expanded)\n",
        "\n",
        "print(\"Valor da Loss utilizando BCELoss:\", loss_bce.item())\n",
        "\n",
        "# Calcular a Loss utilizando a função de BCEWithLogitsLoss\n",
        "loss_bce_with_logits = bce_with_logits_loss_function(logits.squeeze(), targets.float())\n",
        "\n",
        "print(\"Valor da Loss utilizando BCEWithLogitsLoss:\", loss_bce_with_logits.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device.type == 'cuda':\n",
        "    print('GPU:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "else:\n",
        "    print('using CPU')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Defina o modelo para o modo de avaliação\n",
        "model = OneHotMLP(vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "# Extraia o primeiro batch dos dados de treinamento\n",
        "inputs, targets = next(iter(train_loader))\n",
        "inputs = inputs.to(device)\n",
        "targets = targets.to(device)\n",
        "\n",
        "# Faça a predição para cada amostra no batch\n",
        "with torch.no_grad():\n",
        "    logits = model(inputs)\n",
        "    probabilities = torch.sigmoid(logits)\n",
        "    predicted_classes = (probabilities > 0.5).float()\n",
        "\n",
        "# Compare as classes previstas com os targets esperados para calcular a acurácia\n",
        "correct += (predicted_classes == targets.unsqueeze(1)).sum().item()\n",
        "total += targets.size(0)\n",
        "\n",
        "# Calcule a acurácia\n",
        "accuracy = correct / total\n",
        "\n",
        "print(f'Acurácia do primeiro batch: {accuracy * 100:.2f}%')\n",
        "\n",
        "predicted = torch.round(torch.sigmoid(logits.squeeze()))\n",
        "total = targets.size(0)\n",
        "correct = (predicted == targets).sum().item()\n",
        "print(100*correct/total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defina o modelo para o modo de avaliação\n",
        "model = OneHotMLP(vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "# Extraia o primeiro batch dos dados de treinamento\n",
        "inputs, targets = next(iter(train_loader))\n",
        "inputs = inputs.to(device)\n",
        "targets = targets.to(device)\n",
        "\n",
        "\n",
        "# Calcular as previsões do modelo para o primeiro batch\n",
        "logits = model(inputs)\n",
        "probabilities = torch.sigmoid(logits)\n",
        "\n",
        "# Calcular a Loss utilizando a função de entropia cruzada\n",
        "loss = - (targets * torch.log(probabilities) + (1 - targets) * torch.log(1 - probabilities))\n",
        "total_loss = torch.mean(loss)\n",
        "\n",
        "print(\"Valor da Loss:\", total_loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tamanho dos pesos e biases da camada fc1\n",
        "size_fc1_weight = model.fc1.weight.numel()\n",
        "size_fc1_bias = model.fc1.bias.numel()\n",
        "\n",
        "# Tamanho dos pesos e biases da camada fc2\n",
        "size_fc2_weight = model.fc2.weight.numel()\n",
        "size_fc2_bias = model.fc2.bias.numel()\n",
        "\n",
        "# Total de parâmetros do modelo\n",
        "total_params_fc1 = size_fc1_weight + size_fc1_bias\n",
        "total_params_fc2 = size_fc2_weight + size_fc2_bias\n",
        "total_params = total_params_fc1 + total_params_fc2\n",
        "\n",
        "print(f'Tamanho dos pesos da camada fc1: {size_fc1_weight}, Bias: {size_fc1_bias}')\n",
        "print(f'Tamanho dos pesos da camada fc2: {size_fc2_weight}, Bias: {size_fc2_bias}')\n",
        "print(f'Total de parâmetros do modelo: {total_params}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVAhdFGXDepU"
      },
      "source": [
        "## V - Laço de Treinamento - Otimização da função de Perda pelo Gradiente descendente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaH1Uv3yHih5",
        "outputId": "9b94d84a-6d12-41e6-d8d6-e27cdcb9db52"
      },
      "outputs": [],
      "source": [
        "# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device.type == 'cuda':\n",
        "    print('GPU:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "else:\n",
        "    print('using CPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh_pe8rni93_",
        "outputId": "faed899e-e583-484a-abde-45c1ca0d3881"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "model = OneHotMLP(vocab_size)\n",
        "\n",
        "model = model.to(device)\n",
        "# Define loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.18)\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()  # Start time of the epoch\n",
        "    model.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device) \n",
        "        # Forward pass\n",
        "        logits = model(inputs)\n",
        "        loss = criterion(logits.squeeze(), targets.float())\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    end_time = time.time()  # End time of the epoch\n",
        "    epoch_duration = end_time - start_time  # Duration of epoch\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], \\\n",
        "            Loss: {loss.item():.4f}, \\\n",
        "            Elapsed Time: {epoch_duration:.2f} sec')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwvahen5D1oM"
      },
      "source": [
        "## VI - Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## evaluation\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        logits = model(inputs)\n",
        "        predicted = torch.round(torch.sigmoid(logits.squeeze()))\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    print(f'Test Accuracy: {100 * correct / total}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Calculating the gpu time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "model = model.to(device)\n",
        "# Define loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Variáveis para armazenar os tempos\n",
        "tempo_forward_total = 0\n",
        "tempo_backward_total = 0\n",
        "tempo_total_loop = 0\n",
        "\n",
        "# Contador para controlar os dois loops\n",
        "cont_loop = 0\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    start_time_loop = time.time()  # Hora de início do loop\n",
        "    model.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        # Início do tempo do forward\n",
        "        start_time_forward = time.time()\n",
        "        # Forward pass\n",
        "        logits = model(inputs)\n",
        "        loss = criterion(logits.squeeze(), targets.float())\n",
        "        # Fim do tempo do forward\n",
        "        end_time_forward = time.time()\n",
        "        tempo_forward = end_time_forward - start_time_forward\n",
        "        tempo_forward_total += tempo_forward\n",
        "        \n",
        "        # Início do tempo do backward\n",
        "        start_time_backward = time.time()\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Fim do tempo do backward\n",
        "        end_time_backward = time.time()\n",
        "        tempo_backward = end_time_backward - start_time_backward\n",
        "        tempo_backward_total += tempo_backward\n",
        "        \n",
        "        # Contador do loop\n",
        "        cont_loop += 1\n",
        "        \n",
        "        # Se o contador for igual a 2, saia do loop\n",
        "        if cont_loop == 2:\n",
        "            break\n",
        "    \n",
        "    # Fim do tempo do loop\n",
        "    end_time_loop = time.time()\n",
        "    tempo_total = end_time_loop - start_time_loop\n",
        "    tempo_total_loop += tempo_total\n",
        "    \n",
        "    # Print dos tempos\n",
        "    print(f'Tempo do laço: {tempo_total:.3f} segundos')\n",
        "    print(f'Tempo do forward: {tempo_forward_total:.3f} segundos')\n",
        "    print(f'Tempo do backward: {tempo_backward_total:.3f} segundos')\n",
        "\n",
        "# Calculando a média dos tempos para cada iteração\n",
        "media_tempo_loop = tempo_total_loop / num_epochs\n",
        "media_tempo_forward = tempo_forward_total / num_epochs\n",
        "media_tempo_backward = tempo_backward_total / num_epochs\n",
        "\n",
        "print(f'Média do tempo do laço: {media_tempo_loop:.3f} segundos')\n",
        "print(f'Média do tempo do forward: {media_tempo_forward:.3f} segundos')\n",
        "print(f'Média do tempo do backward: {media_tempo_backward:.3f} segundos')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ia024_kernel",
      "language": "python",
      "name": "ia024_kernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
