{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de sentimento por Engenharia de Prompts\n",
    "\n",
    "- Utilizar o groq.com para usar a API do Llama 3 70B para fazer análise de sentimentos do IMDB. É um enunciado bem livre e vamos acompanhando durante a semana em função dos resultados parciais que vocês conseguem fazer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from groq import Groq\n",
    "from tqdm import tqdm\n",
    "import pyserini\n",
    "import requests\n",
    "import tarfile\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset\n",
    "\n",
    "I am going to use the IIRC Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Downloading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(url, save_path, extract_to):\n",
    "    \"\"\" Baixar um arquivo TAR.GZ de uma URL e extrair seu conteúdo.\n",
    "    Argumentos:\n",
    "    url -- URL do arquivo TAR.GZ para download\n",
    "    save_path -- caminho para salvar o arquivo TAR.GZ\n",
    "    extract_to -- diretório para extrair os conteúdos do arquivo TAR.GZ\n",
    "    \"\"\"\n",
    "    # Fazendo o download do arquivo\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.raw.read())\n",
    "        print(\"Download do arquivo completo.\")\n",
    "\n",
    "        # Extraindo o arquivo\n",
    "        if save_path.endswith('.tgz'):\n",
    "            with tarfile.open(save_path, 'r:gz') as tar:\n",
    "                tar.extractall(path=extract_to)\n",
    "            print(\"Extração completa.\")\n",
    "    else:\n",
    "        print(\"Falha no download do arquivo. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL do dataset IIRC\n",
    "url = 'https://iirc-dataset.s3.us-west-2.amazonaws.com/iirc_train_dev.tgz'\n",
    "\n",
    "# Caminho para salvar o arquivo .tgz\n",
    "save_path = '/workspace/aimsbirdclef/ia024/iirc_train_dev.tgz'\n",
    "\n",
    "# Diretório para extrair os conteúdos do arquivo .tgz\n",
    "extract_to = '/workspace/aimsbirdclef/ia024/'\n",
    "\n",
    "# Chamar a função de download e extração\n",
    "#download_and_extract(url, save_path, extract_to)\n",
    "\n",
    "train_json_path = '/workspace/aimsbirdclef/ia024/iirc_train_dev/train.json'\n",
    "dev_json_path = '/workspace/aimsbirdclef/ia024/iirc_train_dev/dev.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "questions    [{'context': [{'text': 'In 1984, a revised pro...\n",
       "links        [{'indices': [202, 229], 'target': 'Leicester ...\n",
       "text         The musical was revived in 1941, 1945 and 1949...\n",
       "title                                           Me and My Girl\n",
       "pid                                                      p_149\n",
       "Name: 149, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_json(train_json_path).head(150)\n",
    "df_train.iloc[149]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Groq\n",
    "\n",
    "Groq's API stands out for its speed and efficiency, making it a viable option for developers looking to implement real-time interactions with LLMs in their applications. To use the Groq API, developers need to install the relevant client libraries and set up their API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Client Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = \"x\"\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Groq Llama 3 testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(review, prompt):\n",
    "    # organizing prompt\n",
    "    prompt = f\"\"\"{prompt}. This is the movie review: '{review}'. \"\"\"\n",
    "    \n",
    "    # Chamando a API com o prompt\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        model=\"llama3-70b-8192\")\n",
    "        \n",
    "    # Retornando a resposta do modelo\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eu classificaria essa review como negativa. Embora o autor use a palavra \"normal\", que pode soar neutra, as outras declarações da review são negativas. \"Little engaging\" (pouco atraente) e \"little well-developed\" (pouco desenvolvidas) sugerem que o filme teve problemas em capturar a atenção do espectador e em criar personagens interessantes. Além disso, a falta de entusiasmo e a linguagem utilizada sugerem que o autor não ficou impressionado com o filme.\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de uso\n",
    "review = \"The movie was normal! The story was little engaging and the characters were little well-developed.\"\n",
    "print(sentiment_analysis(review, 'classifique se essa review de filme é positiva ou negativa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visconde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_without_bar(dataset, prompt):\n",
    "    predictions = [sentiment_analysis(review=review, prompt = prompt) for review in dataset['text']]\n",
    "    actual_labels = ['positive' if label == 1 else 'negative' for label in dataset['label']]\n",
    "    \n",
    "    correct_predictions = sum([pred == true for pred, true in zip(predictions, actual_labels)])\n",
    "    accuracy = correct_predictions*100 / len(dataset)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(dataset, prompt):\n",
    "    \"\"\"Evaluates the accuracy of sentiment analysis on a dataset with a progress bar.\"\"\"\n",
    "    predictions = []\n",
    "    actual_labels = ['positive' if label == 1 else 'negative' for label in dataset['label']]\n",
    "    \n",
    "    # Process each review in the dataset and update the progress bar\n",
    "    for review in tqdm(dataset['text'], desc=\"Analyzing Sentiments\"):\n",
    "        prediction = sentiment_analysis(review=review, prompt=prompt)\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    # Calculate the number of correct predictions\n",
    "    correct_predictions = sum(pred == true for pred, true in zip(predictions, actual_labels))\n",
    "    accuracy = correct_predictions * 100 / len(dataset)\n",
    "    return f\"{accuracy:.4f}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot Accuracy: 95.0500\n"
     ]
    }
   ],
   "source": [
    "# Calculando a acurácia Zero-Shot\n",
    "zeroshot_accuracy = evaluate_accuracy(train_dataset, prompt_zero_shot)\n",
    "print(f\"Zero-Shot Accuracy: {zeroshot_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Sentiments: 100%|██████████| 926/926 [1:14:44<00:00,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Accuracy: 94.6004%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculando a acurácia Few-Shot\n",
    "fewshot_accuracy = evaluate_accuracy(train_dataset, prompt_few_shot)\n",
    "print(f\"Few-Shot Accuracy: {fewshot_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Sentiments: 100%|██████████| 926/926 [1:29:50<00:00,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain-of-Thoughts Accuracy: 94.4924%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculando a acurácia Chain-of-Thoughts\n",
    "cot_accuracy = evaluate_accuracy(train_dataset, prompt_chain_of_thought_cot)\n",
    "print(f\"Chain-of-Thoughts Accuracy: {cot_accuracy}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
